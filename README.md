# üß† Diarisation + R√©sum√© conversationnel

Une application Streamlit avanc√©e qui permet de traiter des fichiers audio pour identifier automatiquement les diff√©rents locuteurs (diarisation) et g√©n√©rer un r√©sum√© intelligent de la conversation avec IA locale.

## ‚ú® Fonctionnalit√©s

### üéØ **Diarisation intelligente**

* Identification automatique des locuteurs avec pyannote.audio 3.1
* Configuration flexible du nombre de locuteurs (min/max)
* Support de multiples formats audio (WAV, MP3, M4A, FLAC)
* Traitement et normalisation automatique des fichiers audio

### ü§ñ **R√©sum√© IA local**

* G√©n√©ration de r√©sum√©s via Ollama (100% local, aucune donn√©e externe)
* Support de multiples mod√®les (Llama3, Mistral, CodeLlama)
* Prompts optimis√©s pour l'analyse conversationnelle
* T√©l√©chargement des r√©sum√©s au format texte

### üé® **Interface utilisateur moderne**

* Interface web intuitive avec Streamlit
* Barre de progression temps r√©el
* Statistiques d√©taill√©es (locuteurs, segments, dur√©e)
* Configuration avanc√©e via sidebar
* Test de connexion int√©gr√© pour Ollama

### üîß **Fonctionnalit√©s techniques**

* Traitement audio automatique (mono, 16kHz, normalisation)
* Gestion robuste des erreurs avec messages informatifs
* Nettoyage automatique des fichiers temporaires
* Logging d√©taill√© pour le d√©bogage
* Architecture modulaire et extensible

## üõ†Ô∏è Technologies utilis√©es

* **[Streamlit](https://streamlit.io/)** : Interface web interactive
* **[pyannote.audio 3.1](https://github.com/pyannote/pyannote-audio)** : Pipeline de diarisation state-of-the-art
* **[Ollama](https://ollama.ai/)** : Mod√®les de langage locaux pour la g√©n√©ration de r√©sum√©s
* **[PyTorch](https://pytorch.org/)** & **[torchaudio](https://pytorch.org/audio/)** : Traitement audio avanc√©
* **[Hugging Face Hub](https://huggingface.co/docs/huggingface_hub)** : Acc√®s aux mod√®les pr√©-entra√Æn√©s

## üìã Pr√©requis

### Logiciels requis

* **Python 3.8+** avec pip
* **Ollama** install√© et configur√© localement
* **Compte Hugging Face** avec acc√®s au mod√®le de diarisation

### Mod√®les Ollama recommand√©s

```bash
# Mod√®les sugg√©r√©s (choisir selon vos besoins)
ollama pull llama3        # Recommand√© pour un bon √©quilibre performance/qualit√©
ollama pull mistral       # Plus rapide, bon pour des r√©sum√©s courts
ollama pull llama2        # Alternative stable
ollama pull codellama     # Sp√©cialis√© pour des conversations techniques
```

### Acc√®s Hugging Face

1. Cr√©ez un compte sur [Hugging Face](https://huggingface.co/)
2. G√©n√©rez un token d'acc√®s dans vos param√®tres
3. Demandez l'acc√®s au mod√®le [pyannote/speaker-diarization](https://huggingface.co/pyannote/speaker-diarization)

## üöÄ Installation

### Installation rapide

```bash
# Clonez le repository
git clone <url-du-repository>
cd diarisation-resume

# Cr√©ez un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installez les d√©pendances
pip install -r requirements.txt
```

### Installation manuelle des d√©pendances

```bash
pip install streamlit>=1.28.0 torch>=2.0.0 torchaudio>=2.0.0 
pip install pyannote.audio>=3.1.0 huggingface_hub>=0.16.0 requests>=2.31.0
```

## üéØ Utilisation

### D√©marrage rapide

1. **Lancez Ollama** (dans un terminal s√©par√©) :

```bash
ollama serve
```

2. **D√©marrez l'application** :

```bash
streamlit run app.py
```

3. **Acc√©dez √† l'interface** : Ouvrez `http://localhost:8501` dans votre navigateur

### Workflow complet

1. **‚öôÔ∏è Configuration** (sidebar) :
   * Saisissez votre token Hugging Face
   * Ajustez le nombre de locuteurs (min/max)
   * Choisissez votre mod√®le Ollama
   * Testez la connexion Ollama
2. **üé§ Upload audio** :
   * Glissez-d√©posez votre fichier (max 100MB)
   * V√©rifiez les informations du fichier
   * Cliquez sur "D√©marrer l'analyse"
3. **üìä Analyse** :
   * Suivez le progr√®s en temps r√©el
   * Consultez les statistiques de diarisation
   * Explorez les segments d√©taill√©s
4. **üìù R√©sum√©** :
   * G√©n√©rez le r√©sum√© avec le mod√®le choisi
   * T√©l√©chargez le r√©sum√© au format texte

## üìÅ Structure du projet

```
diarisation-resume/
‚îú‚îÄ‚îÄ app.py                      # Application principale Streamlit
‚îú‚îÄ‚îÄ config.py                   # Configuration centralis√©e
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ diarization.py         # Module de diarisation audio
‚îÇ   ‚îú‚îÄ‚îÄ summarizer.py          # Module de g√©n√©ration de r√©sum√©s
‚îÇ   ‚îú‚îÄ‚îÄ audio_processor.py     # Traitement et normalisation audio
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py          # Exceptions personnalis√©es
‚îú‚îÄ‚îÄ temp/                      # Fichiers temporaires (auto-cr√©√©)
‚îú‚îÄ‚îÄ logs/                      # Logs de l'application (auto-cr√©√©)
‚îî‚îÄ‚îÄ README.md                  # Documentation
```

## ‚öôÔ∏è Configuration avanc√©e

### Variables d'environnement

```bash
# Optionnel: configurez via variables d'environnement
export HF_TOKEN="votre_token_hugging_face"
export OLLAMA_BASE_URL="http://localhost:11434"
export DEFAULT_OLLAMA_MODEL="llama3"
```

### Personnalisation des mod√®les

Modifiez `config.py` pour ajouter de nouveaux mod√®les :

```python
HF_MODELS = {
    "diarization": "pyannote/speaker-diarization-3.1",
    "diarization_fallback": "pyannote/speaker-diarization@2.1"
}
```

### Optimisation des performances

* **GPU** : PyTorch utilisera automatiquement CUDA si disponible
* **M√©moire** : Ajustez `max_file_size_mb` dans `config.py`
* **Processeur** : Ollama utilise tous les c≈ìurs disponibles

## üêõ R√©solution de probl√®mes

### Erreurs communes et solutions

| Probl√®me                                       | Solution                                            |
| ----------------------------------------------- | --------------------------------------------------- |
| **"Ollama n'est pas accessible"**         | V√©rifiez qu'Ollama est d√©marr√© :`ollama serve` |
| **"√âchec du chargement de la pipeline"** | V√©rifiez votre token HF et l'acc√®s au mod√®le     |
| **"Fichier audio invalide"**              | Convertissez en WAV/MP3 avec un outil externe       |
| **"Mod√®le non trouv√©"**                 | Installez le mod√®le :`ollama pull llama3`        |
| **Erreur de m√©moire**                    | R√©duisez la taille du fichier audio                |

### Diagnostic automatique

L'application inclut des outils de diagnostic :

* **Test de connexion Ollama** depuis l'interface
* **Validation automatique** des fichiers audio
* **Messages d'erreur d√©taill√©s** avec suggestions
* **Logs d√©taill√©s** dans le dossier `logs/`

### Logs et d√©bogage

```bash
# Consultez les logs pour plus de d√©tails
tail -f logs/app.log

# Activez le mode debug (plus verbeux)
export STREAMLIT_LOGGER_LEVEL=debug
streamlit run app.py
```

## üîí S√©curit√© et confidentialit√©

### Protection des donn√©es

* **Traitement 100% local** : Aucune donn√©e n'est envoy√©e vers des serveurs externes
* **Tokens s√©curis√©s** : Les tokens HF sont trait√©s en mode password
* **Fichiers temporaires** : Suppression automatique apr√®s traitement
* **Pas de stockage** : Aucune donn√©e persistante stock√©e

### Recommandations

* Utilisez des tokens HF avec permissions minimales
* Ne partagez jamais vos tokens dans le code
* V√©rifiez r√©guli√®rement les acc√®s √† vos mod√®les HF

## üìä Benchmarks et performances

### Temps de traitement moyens

| Dur√©e audio | Diarisation | R√©sum√© | Total  |
| ------------ | ----------- | -------- | ------ |
| 1 minute     | ~15s        | ~5s      | ~20s   |
| 5 minutes    | ~45s        | ~10s     | ~55s   |
| 15 minutes   | ~2min       | ~15s     | ~2m15s |

*Mesures sur CPU Intel i7, 16GB RAM, mod√®le Llama3*

### Qualit√© des r√©sultats

* **Pr√©cision diarisation** : 85-95% selon la qualit√© audio
* **Qualit√© r√©sum√©s** : D√©pend du mod√®le Ollama choisi
* **Langues support√©es** : Fran√ßais, Anglais (principalement)

## ü§ù Contribution

### Comment contribuer

1. **Forkez** le projet
2. **Cr√©ez une branche** : `git checkout -b feature/nouvelle-fonctionnalite`
3. **D√©veloppez** en suivant les conventions du projet
4. **Testez** votre code avec diff√©rents fichiers audio
5. **Committez** : `git commit -am 'Ajout nouvelle fonctionnalit√©'`
6. **Push** : `git push origin feature/nouvelle-fonctionnalite`
7. **Ouvrez une Pull Request** avec description d√©taill√©e

### Zones d'am√©lioration prioritaires

* [ ] Support de la transcription compl√®te (speech-to-text)
* [ ] Interface multi-langues
* [ ] Export en formats multiples (PDF, JSON, CSV)
* [ ] Traitement par batch
* [ ] API REST pour int√©gration externe

## üîÑ Roadmap

### Version 2.0 (Q3 2025)

* [ ] **Transcription compl√®te** : Ajout de Whisper pour la transcription
* [ ] **Interface multi-langues** : Support FR/EN/ES
* [ ] **Templates de r√©sum√©** : Formats pr√©d√©finis (r√©union, interview, etc.)
* [ ] **Export avanc√©** : PDF avec graphiques, JSON structur√©

### Version 2.1 (Q4 2025)

* [ ] **Traitement batch** : Plusieurs fichiers simultan√©ment
* [ ] **API REST** : Int√©gration dans d'autres applications
* [ ] **Dashboard analytics** : Statistiques d'utilisation
* [ ] **Plugins Ollama** : Support de nouveaux mod√®les

### Version 3.0 (2026)

* [ ] **IA conversationnelle** : Chat avec le contenu audio
* [ ] **D√©tection d'√©motions** : Analyse du sentiment
* [ ] **Int√©gration cloud** : Support services cloud optionnel

## üìù Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

**Utilisation commerciale autoris√©e** sous r√©serve de respect de la licence.

## üÜò Support et communaut√©

### Obtenir de l'aide

* **üêõ Bugs** : Ouvrez une [issue GitHub](https://github.com/votre-repo/issues)
* **üí° Suggestions** : Utilisez les [discussions GitHub](https://github.com/votre-repo/discussions)
* **üìß Support** : Contactez-nous via [email](mailto:support@example.com)

### Ressources utiles

* [Documentation Streamlit](https://docs.streamlit.io/)
* [Guide pyannote.audio](https://pyannote.github.io/pyannote-audio/)
* [Documentation Ollama](https://github.com/jmorganca/ollama)
* [Mod√®les Hugging Face](https://huggingface.co/models)

### Communaut√©

* **Discord** : [Rejoignez notre serveur](https://discord.gg/example)
* **Twitter** : [@VotreProjet](https://twitter.com/example)
* **Blog** : [Articles techniques](https://blog.example.com)

---

## üåü Remerciements

Merci aux √©quipes de [pyannote](https://github.com/pyannote), [Ollama](https://ollama.ai/), et [Streamlit](https://streamlit.io/) pour leurs outils exceptionnels.

**Cr√©√© avec ‚ù§Ô∏è pour simplifier l'analyse de conversations audio**

---

*Derni√®re mise √† jour : Juin 2025 ‚Ä¢ Version 1.0*
